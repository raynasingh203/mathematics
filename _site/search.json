[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mathematical experiments using code",
    "section": "",
    "text": "A1-Group Project\n\n\n\n\n\n\nA1\n\n\nP5js\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nRayna Singh\n\n\n\n\n\n\n\n\n\n\n\n\nA2\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\nRayna Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "A1-Group Project",
    "section": "",
    "text": "Generative Garden\n\n\nRayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code\n Concept Note   In Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\nGarden Elements & Math Concepts\n\n\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\nProcess & Practise\n\n\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  },
  {
    "objectID": "posts/post-with-code/index.html#a1.",
    "href": "posts/post-with-code/index.html#a1.",
    "title": "A1-Group Project",
    "section": "",
    "text": "Generative Garden\n\n\nRayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code\n Concept Note   In Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\nGarden Elements & Math Concepts\n\n\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\nProcess & Practise\n\n\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A2",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\n[Trial ZIP file added separetly] Download Train ZIP file Download Test ZIP file\nOur a2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\nProcess\nWe started by collecting images for each mudra. It was honestly tricky because some friends found it hard to pose like, they literally couldn’t bend their fingers that way.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js.\nBefore jumping into handpose, we also tried out Feature Extractor with MobileNet. Feature Extractor uses MobileNet, which is a pre-trained ML model. After some trial and error and getting lost a bit, we sat down and made a proper step-by-step list to get clarity:\n\nFinish clicking and labeling all the images using our naming format.\nUse Python to generate a CSV with image paths and labels.\nUse a JS sketch to extract 21 keypoints for each image and generate a new CSV.\nTrain the model using this second CSV.\nTest the model with webcam input.\nAdd a song trigger for each mudra.\n\nHandpose detection on a single image worked perfectly we were able to extract 21 points and got excited that we could download a CSV and it was actually working.\nCreating the first CSV using Python was pretty simple.\nThe challenge started when we wrote a JS sketch to read that CSV and output the 21 keypoints for all the images. Initially, it was only giving us values for one image, or just the headings (x1, y1… x21, y21, label) but no actual values. We figured this was because the function to read all rows was in preload(), which runs too early the model takes time to train and wasn’t ready yet, so it only had time to process the first row. Once we moved that part to setup(), everything worked fine and we got the full CSV with coordinates. Another small mistake was that we were usign an older ml5js verson and that was causing a few errors.\nTraining\nA few images just wouldn’t get detected, especially ones where the hands had paint or black charcoal. We had to delete those.\nAll our training and testing so far was with our own dataset and images. So the next step was to combine other teams’ CSVs and train the model together with all of them, then link the mudras to songs.\nWhen we trained just using our team’s images, everything worked fine. But once we started combining datasets, label issues came up.\nFor example, “Arala” vs “arala” were treated as different classes, same with “katakamukha” vs “kataka-mukha”. While combining, we also saw that the pixel value ranges were totally different. One group had values ranging from 500 to 1000, while ours was more like 160 to 300. That’s when we realized we’d need to normalize the values before training.\nTrying varying epochs: 100 to 150 was enough for the samples that we have. We tried to understand the loss graph and we used it like a guide to find the sweet spot between the underfitting and overfitting. Epochs and batch size - It happens automatically if we don’t specify, and we didn’t do that because ml5 internally passes control to TensorFlow.js which sets a default batch size of 32..\n\nOnce we sorted all of that, the last step was adding the songs."
  },
  {
    "objectID": "posts/welcome/index.html#a2.",
    "href": "posts/welcome/index.html#a2.",
    "title": "A2",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\n[Trial ZIP file added separetly] Download Train ZIP file Download Test ZIP file\nOur a2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\nProcess\nWe started by collecting images for each mudra. It was honestly tricky because some friends found it hard to pose like, they literally couldn’t bend their fingers that way.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js.\nBefore jumping into handpose, we also tried out Feature Extractor with MobileNet. Feature Extractor uses MobileNet, which is a pre-trained ML model. After some trial and error and getting lost a bit, we sat down and made a proper step-by-step list to get clarity:\n\nFinish clicking and labeling all the images using our naming format.\nUse Python to generate a CSV with image paths and labels.\nUse a JS sketch to extract 21 keypoints for each image and generate a new CSV.\nTrain the model using this second CSV.\nTest the model with webcam input.\nAdd a song trigger for each mudra.\n\nHandpose detection on a single image worked perfectly we were able to extract 21 points and got excited that we could download a CSV and it was actually working.\nCreating the first CSV using Python was pretty simple.\nThe challenge started when we wrote a JS sketch to read that CSV and output the 21 keypoints for all the images. Initially, it was only giving us values for one image, or just the headings (x1, y1… x21, y21, label) but no actual values. We figured this was because the function to read all rows was in preload(), which runs too early the model takes time to train and wasn’t ready yet, so it only had time to process the first row. Once we moved that part to setup(), everything worked fine and we got the full CSV with coordinates. Another small mistake was that we were usign an older ml5js verson and that was causing a few errors.\nTraining\nA few images just wouldn’t get detected, especially ones where the hands had paint or black charcoal. We had to delete those.\nAll our training and testing so far was with our own dataset and images. So the next step was to combine other teams’ CSVs and train the model together with all of them, then link the mudras to songs.\nWhen we trained just using our team’s images, everything worked fine. But once we started combining datasets, label issues came up.\nFor example, “Arala” vs “arala” were treated as different classes, same with “katakamukha” vs “kataka-mukha”. While combining, we also saw that the pixel value ranges were totally different. One group had values ranging from 500 to 1000, while ours was more like 160 to 300. That’s when we realized we’d need to normalize the values before training.\nTrying varying epochs: 100 to 150 was enough for the samples that we have. We tried to understand the loss graph and we used it like a guide to find the sweet spot between the underfitting and overfitting. Epochs and batch size - It happens automatically if we don’t specify, and we didn’t do that because ml5 internally passes control to TensorFlow.js which sets a default batch size of 32..\n\nOnce we sorted all of that, the last step was adding the songs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]