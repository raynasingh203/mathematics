[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mathematical experiments using code",
    "section": "",
    "text": "╰┈➤ A1\n\n\n\n\n\n\nA1\n\n\np5.js\n\n\n\n\n\n\n\n\n\nMay 31, 2025\n\n\nRayna Singh\n\n\n\n\n\n\n\n\n\n\n\n\n╰┈➤ A2\n\n\n\n\n\n\nA2\n\n\np5.js\n\n\nml5.js\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\nRayna Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "╰┈➤ A1",
    "section": "",
    "text": "Generative Garden\n\n\nRayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code\n Concept Note   In Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\nGarden Elements & Math Concepts\n\n\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\nProcess & Practise\n\n\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  },
  {
    "objectID": "posts/post-with-code/index.html#a1.",
    "href": "posts/post-with-code/index.html#a1.",
    "title": "A1-Group Project",
    "section": "",
    "text": "Generative Garden\n\n\nRayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code\n Concept Note   In Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\nGarden Elements & Math Concepts\n\n\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\nProcess & Practise\n\n\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "╰┈➤ A2",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\nOur A2 project explores how a neural network can recognize five classical mudras using hand gesture detection, triggering a unique song for each one. \nWe combined hand pose estimation with a custom-trained neural network to build an interactive system that responds to traditional gestures in real time.\n\n\nFiles\n\n\n\nTrial ZIP File [added separately on brightspace]\n\nImage datasets\n\nCSV generation codes\n\nPython scripts for cleaning the data\n\nTest outputs (csvs again) from extracting keypoints\n\nTrain ZIP File Download Train ZIP file\n\nFinal, cleaned and combined CSV\n\nCode used to train the model with ml5.neuralNetwork\n\nTrained model files (model.json and weights)\n\nTest ZIP File Download Test ZIP file\n\nFinal testing code in p5.js and ml5.js\n\nVersion used during the final demo\n\n\n\n\nProcess et cetera\n\n\nWe started by collecting images for each mudra. Honestly, it wasn’t easy, some people struggled to hold the poses correctly. Their fingers wouldn’t bend the right way, and we had to guide them through the gestures.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js. We also briefly tested FeatureExtractor using MobileNet, a pre-trained convolutional neural network. While that worked well for basic image classification, it wasn’t ideal for our keypoint-based approach.\nOur process became more structured after we broke it down step by step:\n1– Click and label all mudra images using our naming format.\n2– Use Python to generate a CSV of file paths and mudra labels.\n3– Use a JavaScript sketch to extract 21 keypoints using handpose.\n4– Save those as a second CSV with 42 values (x, y for each keypoint).\n5– Train the model using ml5’s NeuralNetwork class.\n6– Trigger specific audio clips based on predictions.\nWe were thrilled when handpose detection worked on a single image. Seeing 21 points plotted and successfully exported to a CSV felt like a major win. The real challenge came when we tried to extract keypoints from multiple images. Initially, only the first image would return valid landmark data or in some runs, just the header row appeared in the output CSV. After some debugging, we realized the handpose model was being called before it had fully loaded. We had placed the loop logic inside preload(), which runs before asynchronous models like ml5.handpose() are ready. Once we moved the loop and prediction logic into setup() and ensured it only ran after the model had finished loading, all images were processed correctly.\nWe also ran into silent failures because we were using an older version of ml5.js. Updating to v0.6.0 solved most of these issues. All of this was done using just our own dataset—and by the end of it, we had a clean, functioning CSV and a working model that could accurately detect our mudras. It gave us a solid foundation before moving on to combining data from other teams.\nMerging datasets from other teams introduced a few complications:\n–Label inconsistencies like “Arala” vs “arala”, or “kataka-mukha” vs “katakamukha”.\n–Range mismatches: One team’s landmark coordinates ranged between 500–1000, while ours were around 160–300. We cleaned the labels for consistency by writing py scripts and weren’t really sure if the range mismatch would pose a huge problem, we found that normalising in our final code would help solve that problem so that’s exactly what we did.\nDuring training, we used a neural network architecture with two dense (fully connected) layers. The first hidden layer contains 16 neurons, and the final output layer contains 6 neurons, one for each mudra class. This reduction from 16 to 6 happens because the first layer transforms the 42 landmark inputs into 16 abstract features, and the second layer uses those features to predict the probabilities of the 6 possible mudras.The first layer learns patterns, and the second layer classifies them.\nThe training ran for 100 epochs using ml5.js’s built-in training system. As shown in the training performance/loss graph, the loss started high (around 1.7) and steadily decreased throughout training, eventually plateauing around 0.35–0.40. This trend indicated that the model was successfully learning from the dataset without overfitting. There are some fluctuations in the mid-epochs (around epoch 55), which could be due to variations in gesture quality or class imbalance, but overall, we thought the model stabilised well.\nWe were also unsure whether we needed to manually set a batch size along with the number of epochs. After some research, we found that ml5.js handles batching internally by passing control to TensorFlow.js, which sets a default batch size of 32 if none is provided. Since our dataset wasn’t extremely extremelyyy large, we decided to leave it at the default, and training worked smoothly. \nWhile testing the system, we realised that the model was always giving a prediction, even when the hand wasn’t clearly showing a mudra. This sometimes led to incorrect or low-confidence outputs being treated as valid. To make the system feel more trustworthy, we decided to display the confidence score on screen. This way, users could see how sure the model was about each prediction, even if it wasn’t perfect.\nWe considered adding a confidence threshold that would prevent any feedback below a certain score, but we realised that would make the system feel less responsive. Instead, we chose to keep sound playback separate from confidence filtering. So even if confidence is low, the sound still plays but the user can see that the system is unsure. It was more about transparency than restriction. At first, the sound logic itself had issues. Different tracks would play over each other or cut off suddenly when the predictions changed rapidly. We solved this by stopping all currently playing sounds before triggering a new one, and by making sure that a sound only plays when the predicted mudra actually changes.\nFinally, we noticed the model was sometimes predicting a mudra even when there was no hand in the frame at all. To deal with that, we added a quick check so that classification only runs if a hand is actually detected using handpose. No hand, no prediction, no sound. It made the system feel way more grounded and less random.\n\n\nResults"
  },
  {
    "objectID": "posts/welcome/index.html#a2.",
    "href": "posts/welcome/index.html#a2.",
    "title": "A2",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\n[Trial ZIP file added separetly] Download Train ZIP file Download Test ZIP file\nOur a2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\nProcess\nWe started by collecting images for each mudra. It was honestly tricky because some friends found it hard to pose like, they literally couldn’t bend their fingers that way.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js.\nBefore jumping into handpose, we also tried out Feature Extractor with MobileNet. Feature Extractor uses MobileNet, which is a pre-trained ML model. After some trial and error and getting lost a bit, we sat down and made a proper step-by-step list to get clarity:\n\nFinish clicking and labeling all the images using our naming format.\nUse Python to generate a CSV with image paths and labels.\nUse a JS sketch to extract 21 keypoints for each image and generate a new CSV.\nTrain the model using this second CSV.\nTest the model with webcam input.\nAdd a song trigger for each mudra.\n\nHandpose detection on a single image worked perfectly we were able to extract 21 points and got excited that we could download a CSV and it was actually working.\nCreating the first CSV using Python was pretty simple.\nThe challenge started when we wrote a JS sketch to read that CSV and output the 21 keypoints for all the images. Initially, it was only giving us values for one image, or just the headings (x1, y1… x21, y21, label) but no actual values. We figured this was because the function to read all rows was in preload(), which runs too early the model takes time to train and wasn’t ready yet, so it only had time to process the first row. Once we moved that part to setup(), everything worked fine and we got the full CSV with coordinates. Another small mistake was that we were usign an older ml5js verson and that was causing a few errors.\nTraining\nA few images just wouldn’t get detected, especially ones where the hands had paint or black charcoal. We had to delete those.\nAll our training and testing so far was with our own dataset and images. So the next step was to combine other teams’ CSVs and train the model together with all of them, then link the mudras to songs.\nWhen we trained just using our team’s images, everything worked fine. But once we started combining datasets, label issues came up.\nFor example, “Arala” vs “arala” were treated as different classes, same with “katakamukha” vs “kataka-mukha”. While combining, we also saw that the pixel value ranges were totally different. One group had values ranging from 500 to 1000, while ours was more like 160 to 300. That’s when we realized we’d need to normalize the values before training.\nTrying varying epochs: 100 to 150 was enough for the samples that we have. We tried to understand the loss graph and we used it like a guide to find the sweet spot between the underfitting and overfitting. Epochs and batch size - It happens automatically if we don’t specify, and we didn’t do that because ml5 internally passes control to TensorFlow.js which sets a default batch size of 32..\n\nOnce we sorted all of that, the last step was adding the songs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html#a2",
    "href": "posts/welcome/index.html#a2",
    "title": "╰┈➤ A2",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\nOur a2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\n\n\nFiles\n\n\n\nTrial ZIP File [added separately on brightspace]\n\nImage datasets\n\nCSV generation codes\n\nPython scripts for cleaning the data\n\nTest outputs (csvs again) from extracting keypoints\n\nTrain ZIP File Download Train ZIP file\n\nFinal, cleaned and combined CSV\n\nCode used to train the model with ml5.neuralNetwork\n\nTrained model files (model.json and weights)\n\nTest ZIP File Download Test ZIP file\n\nFinal testing code in p5.js and ml5.js\n\nVersion used during the final demo\n\n\n\n\nProcess et cetera\n\n\nWe started by collecting images for each mudra. Honestly, it wasn’t easy, some people struggled to hold the poses correctly. Their fingers wouldn’t bend the right way, and we had to guide them through the gestures.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js. We also briefly tested FeatureExtractor using MobileNet, a pre-trained convolutional neural network. While that worked well for basic image classification, it wasn’t ideal for our keypoint-based approach.\nOur process became more structured after we broke it down step by step:\n1– Click and label all mudra images using our naming format.\n2– Use Python to generate a CSV of file paths and mudra labels.\n3– Use a JavaScript sketch to extract 21 keypoints using handpose.\n4– Save those as a second CSV with 42 values (x, y for each keypoint).\n5– Train the model using ml5’s NeuralNetwork class.\n6– Trigger specific audio clips based on predictions.\nWe were thrilled when handpose detection worked on a single image. Seeing 21 points plotted and successfully exported to a CSV felt like a major win. The real challenge came when we tried to extract keypoints from multiple images. Initially, only the first image would return valid landmark data or in some runs, just the header row appeared in the output CSV. After some debugging, we realized the handpose model was being called before it had fully loaded. We had placed the loop logic inside preload(), which runs before asynchronous models like ml5.handpose() are ready. Once we moved the loop and prediction logic into setup() and ensured it only ran after the model had finished loading, all images were processed correctly. We also ran into silent failures because we were using an older version of ml5.js. Updating to v0.6.0 solved most of these issues. All of this was done using just our own dataset—and by the end of it, we had a clean, functioning CSV and a working model that could accurately detect our mudras. It gave us a solid foundation before moving on to combining data from other teams.\nMerging datasets from other teams introduced a few complications:\n–Label inconsistencies like “Arala” vs “arala”, or “kataka-mukha” vs “katakamukha”.\n–Range mismatches: One team’s landmark coordinates ranged between 500–1000, while ours were around 160–300. We cleaned the labels for consistency by writing py scripts and weren’t really sure if the range mismatch would pose a huge problem, we found that normalising in our final code would help solve that problem so that’s exactly what we did.\nDuring training, we used a neural network architecture with two dense (fully connected) layers. The first hidden layer contains 16 neurons, and the final output layer contains 6 neurons, one for each mudra class. This reduction from 16 to 6 happens because the first layer transforms the 42 landmark inputs into 16 abstract features, and the second layer uses those features to predict the probabilities of the 6 possible mudras.The first layer learns patterns, and the second layer classifies them. The training ran for 100 epochs using ml5.js’s built-in training system. As shown in the training performance/loss graph, the loss started high (around 1.7) and steadily decreased throughout training, eventually plateauing around 0.35–0.40. This trend indicated that the model was successfully learning from the dataset without overfitting. There are some fluctuations in the mid-epochs (around epoch 55), which could be due to variations in gesture quality or class imbalance, but overall the model stabilised well. We were also unsure whether we needed to manually set a batch size along with the number of epochs. After some research, we found that ml5.js handles batching internally by passing control to TensorFlow.js, which sets a default batch size of 32 if none is provided. Since our dataset wasn’t extremelyyy large, we decided to leave it at the default, and training worked smoothly. \nWhile testing the system, we realised that the model was always giving a prediction, even when the hand wasn’t clearly showing a mudra. This sometimes led to incorrect or low-confidence outputs being treated as valid. To make the system feel more trustworthy, we decided to display the confidence score on screen. This way, users could see how sure the model was about each prediction, even if it wasn’t perfect. We considered adding a confidence threshold that would prevent any feedback below a certain score, but we realised that would make the system feel less responsive. Instead, we chose to keep sound playback separate from confidence filtering. So even if confidence is low, the sound still plays but the user can see that the system is unsure. It was more about transparency than restriction. At first, the sound logic itself had issues. Different tracks would play over each other or cut off suddenly when the predictions changed rapidly. We solved this by stopping all currently playing sounds before triggering a new one, and by making sure that a sound only plays when the predicted mudra actually changes. Finally, we noticed the model was sometimes predicting a mudra even when there was no hand in the frame at all. To deal with that, we added a quick check so that classification only runs if a hand is actually detected using handpose. No hand, no prediction, no sound. It made the system feel way more grounded and less random.\n\n\nResults"
  },
  {
    "objectID": "posts/post-with-code/index.html#a1",
    "href": "posts/post-with-code/index.html#a1",
    "title": "╰┈➤ A1",
    "section": "",
    "text": "Generative Garden\n\n\nRayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code\n Concept Note   In Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\nGarden Elements & Math Concepts\n\n\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\nProcess & Practise\n\n\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  }
]